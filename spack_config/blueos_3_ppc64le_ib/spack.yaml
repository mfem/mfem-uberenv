spack:

  config:
    install_tree:
      root: $spack/..
      # padded_length: 128
      projections:
        all: "{compiler.name}-{compiler.version}/{name}-{version}-{hash}"
    # Note: unlike projects using RADIUSS Spack
    # Configs, Serac is using "$spack/.." as the root
    # of Spack cache directories. We share the goal of
    # making sure that Spack instances do not collide,
    # our choice supposes that $user_cache_path is
    # defined to that purpose, but Serac is enforcing
    # it for the cache directories.
    # The reason for not adopting the same strategy as
    # Serac is because there are other directories
    # created by Spack that need to be porperly
    # isolated. Therefore, setting the
    # "user_cache_path" properly isstill mandatory.
    build_stage::
      - $user_cache_path/stage
    test_stage: $user_cache_path/test
    misc_cache: $user_cache_path/misc
    flags:
      keep_werror: 'all'

    view: false

  compilers::
  - compiler:
      spec: gcc@=8.3.1
      paths:
        cc: /usr/tce/packages/gcc/gcc-8.3.1/bin/gcc
        cxx: /usr/tce/packages/gcc/gcc-8.3.1/bin/g++
        f77: /usr/tce/packages/gcc/gcc-8.3.1/bin/gfortran
        fc: /usr/tce/packages/gcc/gcc-8.3.1/bin/gfortran
      flags: {}
      operating_system: rhel7
      target: ppc64le
      modules: []
      environment: {}
      extra_rpaths: []
  - compiler:
      spec: xl@=16.1.1.12
      paths:
        cc: /usr/tce/packages/xl/xl-2022.08.19/bin/xlc_r
        cxx: /usr/tce/packages/xl/xl-2022.08.19/bin/xlC_r
        fc: /usr/tce/packages/xl/xl-2022.08.19/bin/xlf2003_r
        f77: /usr/tce/packages/xl/xl-2022.08.19/bin/xlf_r
      flags: {}
      operating_system: rhel7
      target: ppc64le
      modules: []
      environment: {}
      extra_rpaths: []

  packages::
    all:
      # This defaults us to machine specific flags of ivybridge which allows
      # us to run on broadwell as well
      target: [ppc64le]
      compiler: [gcc, xl]
      providers:
        blas: [netlib-lapack]
        lapack: [netlib-lapack]
        mpi: [specturum-mpi]
        gl: [opengl]
        glu: [openglu]

    opengl:
      buildable: false
      externals:
      - spec: opengl@1.7.0
        prefix: /usr

    openglu:
      buildable: false
      externals:
      - spec: openglu@1.3.1
        prefix: /usr

    cuda:
      buildable: false
      # Versions of CUDA > 11.1.1 with XL C++ have issues with the MFEM tests.
      # CUDA > 11.2.0 with XL C++ fails to compile 'catch.hpp' giving errors in
      # STL headers from GCC 4.9.3; CUDA 11.2.0 fails running cunit_tests.
      version: [11.1.1, 10.1.243]
      externals:
      - spec: cuda@11.1.1
        prefix: /usr/tce/packages/cuda/cuda-11.1.1
      - spec: cuda@10.1.243
        prefix: /usr/tce/packages/cuda/cuda-10.1.243

    # Lock down which MPI we are using
    mpi:
      buildable: false
    spectrum-mpi:
      externals:
      - spec: spectrum-mpi@10.3.1.03rtm0%gcc@8.3.1
        prefix: /usr/tce/packages/spectrum-mpi/spectrum-mpi-rolling-release-gcc-8.3.1
      - spec: spectrum-mpi@10.3.1.03rtm0%xl@16.1.1.12
        prefix: /usr/tce/packages/spectrum-mpi/spectrum-mpi-rolling-release-xl-2022.08.19

    netlib-lapack:
      externals:
      - spec: netlib-lapack@3.10.0%xl
        prefix: /usr/tcetmp/packages/lapack/lapack-3.10.0-xl-2022.03.10

  # Dev tools
    cmake:
      buildable: false
      version: [3.23.1]
      externals:
      - spec: cmake@3.23.1
        prefix: /usr/tce/packages/cmake/cmake-3.23.1

    python:
      buildable: false
      version: [3.8.2]
      externals:
      - spec: python@3.8.2
        prefix: /usr/tce/packages/python/python-3.8.2
